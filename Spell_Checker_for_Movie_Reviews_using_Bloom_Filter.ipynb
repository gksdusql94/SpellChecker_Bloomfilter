{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bloom Filters\n",
        "\n",
        "This notebook introduces the `bloom_filter` Python library and compares its performance with other Python data structures.\n",
        "\n",
        "Reference: [Stanford CS 246](https://web.stanford.edu/class/cs246/)"
      ],
      "metadata": {
        "id": "5vE9Tre80vfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install bloom_filter library\n",
        "!pip install bloom_filter\n",
        "\n",
        "# Import a large list of English dictionary works\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.corpus import words\n",
        "word_list = words.words()\n",
        "print(f'Dictionary length: {len(word_list)}')\n",
        "print(word_list[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "365Nos9Y0p5y",
        "outputId": "8079dd87-6c84-449d-ac7f-21c5e11154cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bloom_filter\n",
            "  Downloading bloom_filter-1.3.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading bloom_filter-1.3.3-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: bloom_filter\n",
            "Successfully installed bloom_filter-1.3.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary length: 236736\n",
            "['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a group of movie reviews\n",
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "neg_reviews = []\n",
        "pos_reviews = []\n",
        "\n",
        "for fileid in movie_reviews.fileids('neg'):\n",
        "  neg_reviews.extend(movie_reviews.words(fileid))\n",
        "for fileid in movie_reviews.fileids('pos'):\n",
        "  pos_reviews.extend(movie_reviews.words(fileid))\n",
        "\n",
        "len(word_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV4F8Nbv1n4H",
        "outputId": "9bdac67f-58c6-4e71-e40a-3f1218cff6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "236736"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bloom filter\n",
        "from bloom_filter import BloomFilter\n",
        "\n",
        "word_filter = BloomFilter(max_elements=236736)\n",
        "\n",
        "for word in word_list:\n",
        "  word_filter.add(word)\n",
        "\n",
        "# For comparison, also create a set of words\n",
        "word_set = set(word_list)\n",
        "\n",
        "# Inspect the size of each data structure using getsizeof() method\n",
        "from sys import getsizeof\n",
        "\n",
        "print(f'Size of word_list (in bytes): {getsizeof(word_list)}')\n",
        "\n",
        "print(f'Size of word_filter (in bytes): {getsizeof(word_filter)}')\n",
        "\n",
        "print(f'Size of word_set (in bytes): {getsizeof(word_set)}')\n",
        "\n",
        "# Use %timeit magic command to estimate execution time\n",
        "%timeit \"California\" in word_list\n",
        "%timeit \"California\" in word_filter\n",
        "%timeit \"California\" in word_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9CpGfYq1t51",
        "outputId": "7142ef11-7293-47ca-ed90-bec3e723f9bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word_list (in bytes): 2055512\n",
            "Size of word_filter (in bytes): 48\n",
            "Size of word_set (in bytes): 8388824\n",
            "762 µs ± 412 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
            "30.5 µs ± 12.1 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "191 ns ± 89.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "In this Colab we just need to install a [bloom_filter](https://github.com/hiway/python-bloom-filter), a Python library which offers an implementations of Bloom filters.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qHai2252mI",
        "outputId": "e01853ee-1c94-4f3d-aa3f-9037935c56b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install bloom_filter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bloom_filter in /usr/local/lib/python3.10/dist-packages (1.3.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYRX2PMm0L6"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO_IcxgquzhI"
      },
      "source": [
        "From the NLTK (Natural Language ToolKit) library, we import a large list of English dictionary words, commonly used by the very first spell-checking programs in Unix-like operating systems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xz3f79crEEb",
        "outputId": "96bc1ed1-28f0-43a4-f17b-6d304a6c7995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.corpus import words\n",
        "word_list = words.words()\n",
        "print(f'Dictionary length: {len(word_list)}')\n",
        "print(word_list[:15])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary length: 236736\n",
            "['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csbQXPUFUMob"
      },
      "source": [
        "Then we load another dataset from the NLTK Corpora collection: ```movie_reviews```.\n",
        "\n",
        "The movie reviews are categorized between *positive* and *negative*, so we construct a list of words (usually called **bag of words**) for each category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwgRhMT1UNUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40ebec37-9a08-4df1-c381-04cb9910ef7f"
      },
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "neg_reviews = []\n",
        "pos_reviews = []\n",
        "\n",
        "for fileid in movie_reviews.fileids('neg'):\n",
        "  neg_reviews.extend(movie_reviews.words(fileid))\n",
        "for fileid in movie_reviews.fileids('pos'):\n",
        "  pos_reviews.extend(movie_reviews.words(fileid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrHJptH3Tb-3"
      },
      "source": [
        "In this Colab, we will develop a very simplistic spell-checker.  By no means we should think of using it for a real-world use case, but it is an interesting exercise to highlight the strenghts and weaknesses of Bloom Filters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK3WyXaPsa5q"
      },
      "source": [
        "from bloom_filter import BloomFilter\n",
        "\n",
        "word_filter = BloomFilter(max_elements=236736)\n",
        "\n",
        "for word in word_list:\n",
        "  word_filter.add(word)\n",
        "\n",
        "word_set = set(word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddqg0odiaSRg"
      },
      "source": [
        "If we executed the cell above, we now have 3 different variables in our scope:\n",
        "\n",
        "1.   ```word_list```, a Python list containing the English dictionary (in case insensitive order)\n",
        "2.   ```word_filter```, a Bloom filter where we have already added all the words in the English dictionary\n",
        "3.   ```word_set```, a [Python set](https://docs.python.org/3.6/library/stdtypes.html#set-types-set-frozenset) built from the same list of words in the English dictionary\n",
        "\n",
        "Let's inspect the size of each data structure using the [getsizeof()](https://docs.python.org/3/library/sys.html#sys.getsizeof) method!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVLxu20maRLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4301ecf5-ce0f-4898-9d6f-558a26aa99b0"
      },
      "source": [
        "from sys import getsizeof\n",
        "\n",
        "print(f'Size of word_list (in bytes): {getsizeof(word_list)}')\n",
        "\n",
        "print(f'Size of word_filter (in bytes): {getsizeof(word_filter)}')\n",
        "print(f'Size of word_set (in bytes): {getsizeof(word_set)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word_list (in bytes): 2055512\n",
            "Size of word_filter (in bytes): 48\n",
            "Size of word_set (in bytes): 8388824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbQzd4czlT3h"
      },
      "source": [
        "We should have noticed how efficient is the Bloom filter in terms of memory footprint!\n",
        "\n",
        "Now let's find out how fast is the main operation for which we construct Bloom filters: *membership testing*. To do so, we will use the ```%timeit``` IPython magic command, which times the repeated execution of a single Python statement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq7I6kJfwXy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff4158b-c7fd-4d67-f652-388303d460d6"
      },
      "source": [
        "%timeit -r 3 \"California\" in word_list\n",
        "%timeit \"California\" in word_filter\n",
        "%timeit \"California\" in word_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "555 µs ± 147 µs per loop (mean ± std. dev. of 3 runs, 1000 loops each)\n",
            "16.1 µs ± 3.69 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
            "54.9 ns ± 0.699 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq2LVgEVnI_R"
      },
      "source": [
        "Notice the performance gap between linear search on a list, multiple hash computations in a Bloom filter, and a single hash computation in a native Python ```Set()```.\n",
        "\n",
        "We now have all the building blocks required to build our spell-checker, and we understand the performance tradeoffs of each data structure we chose. Write a function that takes as arguments (1) a list of words, and (2) any of the 3 dictionary data structures we constructed. The function must return the number of words which **do not appear** in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTT-6rQcnibH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab659112-0e03-4bb8-b038-2129b446b41a"
      },
      "source": [
        "def func(a, ds):\n",
        "  cnt = 0\n",
        "  for x in a:\n",
        "    if x not in ds:\n",
        "      cnt += 1\n",
        "      return cnt / len(a)\n",
        "\n",
        "%timeit func(neg_reviews, word_filter)\n",
        "\n",
        "# Example usage with %timeit\n",
        "result_ratio = func(neg_reviews, word_filter)\n",
        "print(f'Ratio of words not in the dictionary: {result_ratio}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.6 µs ± 843 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
            "Ratio of words not in the dictionary: 1.3311041775373508e-06\n"
          ]
        }
      ]
    }
  ]
}